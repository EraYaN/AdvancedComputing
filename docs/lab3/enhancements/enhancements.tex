%!TEX program=xelatex
%!TEX spellcheck=en_GB
\documentclass[final]{report}
\input{../../.library/preamble.tex}
\input{../../.library/style.tex}
\addbibresource{../../.library/bibliography.bib}
\begin{document}
\chapter{Enhancements}\label{ch:enhancements}

\section{Introduction}
The implementation as described in \cref{ch:initopencl} is similar to the first GPU implementation by H.A. Du Nguyen.
This report will consider three enhancement techniques to improve the performance and get it closer to that of the enhanced CUDA version.
The three techniques are:
\begin{itemize}
	\item Eliminating branching divergence with OpenCL Images (Texture memory) (\cref{sec:image-texture}).
	\item Pass \texttt{iApp} as an array (\cref{sec:iapp-array}), since all cells in the network receive a fixed input current at each time step. Since the simulation is short, \texttt{iApp} stays constant throughout the course of the program, setting iApp as a constant is another enhancement.
	\item Merge neighbour and compute into one kernel (\cref{sec:one-kernel})
\end{itemize}

\section{OpenCL Image}\label{sec:image-texture}
OpenCL images are the OpenCL equivalent of the CUDA textures.
This was implemented for the array containing the vDend values for all the cells.
OpenCL offers two options for handling border cases, CLK\_ADDRESS\_CLAMP and CLK\_ADDRESS\_CLAMP\_TO\_EDGE.
For CLK\_ADDRESS\_CLAMP the out of range indexes with return a border colour, and for CLK\_ADDRESS\_CLAMP\_TO\_EDGE the value of the closest edge cell is returned.
CLK\_ADDRESS\_CLAMP in noticeably slower, while you would expect it to be faster.
OpenCL does not support double type images, so we went with CL\_UNSIGNED\_INT32 type images with channel layout CL\_RG.
So when we retrieve a "pixel" from this image we can use the first element of the double2 vector that we cast the returned uint4 to.
This is not very optimal, but switching to CL\_UNSIGNED\_INT16 and CL\_RGBA  did not help, performance wise.
As shown in \cref{tab:gpu-specs} (Stage 2) we even lost a little bit of performance on this enhancement.
Hence why we removed it in Stage 3.

\section{iApp as an array}\label{sec:iapp-array}
The external input current \texttt{iApp} is copied every time step to the GPU.
Since the input current is fixed and known before execution, it can be transferred to the GPU before the actual simulation starts.
In CUDA this is done by passing the input current as an array for each simulation step and generating the index as a global variable.
In OpenCL this approach fails, it does support global variables, however these still need to be passed with a write buffer.
Even though the copying of the input current can be avoided, the index still needs to be copied, which effectively has zero profit.

By simplifying the the approach even more, one can argue that the input current is one constant value for every simulation step as long as not too many simulation steps are being used.
This allows for one constant value on the GPU side, omitting the copying of the input current.

\section{Merged Kernels}\label{sec:one-kernel}
The calling of the kernel was found to be expensive.
In order to reduce the two kernel calls per iteration to one kernel call, the decision was made to merge both kernels into one.
This way we have only one kernel call per loop iteration and kernel swapping, which is another resource intensive operation, is prevented.
Moreover, the neighbour part immediately stores its results in \texttt{compParams} and omits putting it in the state first.
The loop that took care of this has also been removed.

\section{Extra Enhancements}
\subsection{Splitting up the Compute Kernel}
Splitting the kernel into five kernels, setup, dend, soma, axon and finish, has been considered to improve concurrency.
However, kernel swapping is a very expensive operation in OpenCL and the splitting it would add a lot of complexity.
Also most GPU's do not support running multiple kernels concurrently (cl\_ext\_device\_fission) as of yet, and NVidia Kepler certainly does not support it.
This is mostly usable in CPU and some accelerator devices.
Therefore, it was chosen not to implement this enhancement.

\subsection{Using Multiple OpenCL Devices}
An other option is to use multiple OpenCL devices.
On a modern CPU this makes sense, most consumer grade CPU include a fairly capable GPU and are quite capable themselves.
In the server provided to us, the included CPU is very old and it is not worth the additional complexity of inter device communication and the thousands of line of extra OpenCL boilerplate code.
This is a very interesting venue to explore further down the line, but for now it does not make sense.
It would give the program much more scalability, however this is probably easier to implement using CUDA.

\end{document}